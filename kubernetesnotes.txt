components of kubernetes -
All the above components are present on the control plane/leader node.
1. api server
    - when api server gets the request to create a pod the data reg the same is stored in etcd server.
    - once the data is saved etcd server sends response to api server.
2. controller manager
    - controller manager tries to manage the the request to api server to the cluster status/ controller maintains the state.
    - deployments, statefulsets, replicasets are a part of controller manager.
3. scheduler
    - scheduler decides on which particular node to schedule the pod on. based on affinity and node resources.
4. etcd server
    - etcd is a key-value store

member/slave contains the below components
1. kubelet - kubelet waits for the incoming request to its node and talks with container engine on the node to schedule the pod.
2. kube proxy - this helps in inter node communication.

kube-config file for authentiation and talking with api-server.
kubectl talks to api-server, all the authentication and the details of the api-server are stored in kube-config file.

kubectl runs api calls on api-server by locating the api-server by kube-config file.
api-server exposes end points which kubectl uses.
kube-config file contains the api-server details to which kubectl talks

kind - pod, statefulset, deployment
apigroup - similar kind of ojects like storage, networking etc
version - apigroup is in version

gvk - group version kind
gvr - group version resource
with the above two any resource can be targetted.

api-server contains all the apis, these apis get, set the k8s objects.

kubectl proxy
curl <exposed url>
curl <exposed url>/api/v1/namespaces/kube-system/pods

**********************************
Docker - container platform. Docker helps to interact with the container.
Kubernetes - container orchestration platform.
containers are ephimeral(die & revive)

shortcomings of docker:
	Single host to schedule the containers(container can be scheduled on only one host)
	Autohealing - without manual intervention any container which got killed should come up.
	Autoscaling - when the load on the container is maximum, the no of container instances should increase
				  to accomodate the load.
	docker alone is a simple platform which is not enterprise ready, it does not have scaling, healing,
	load balancer, API gateway support, firewall out of the box.
	
Kubernetes solves the above problems as it supports Autohealing, Autoscaling, Multi host setup and Enterprise 
level support.
Multi host setup - Kubernetes is by default a cluster setup. which contains master and worker nodes.
Autoscaling - Replication controller manages the auto scaling either from yml configuration or HPA(horizontal pod
				autoscaler)
Autohealing - whenever any pod goes down kubernetes spins up a new pod even before the other one completely
			  went down, API server is used for this process.
Kubernetes is an enterprise level application.

To start a minikube cluster:
	minikube start --driver=docker
To delete a minikube:
	minikube delete
	
Kubernetes Architecture:
Kubernetes contains master plane and data plane(worker nodes)
master plane contains etcd server, Controller manager, scheduler, API server, cloud controller manager.
data plane contains kubelet and kube-proxy.

Data plane:
kubelet - Request to create a pod goes through the master plane to data plane, kubelet in the dataplane is responsible
for creating the pod and the pod needs a container runtime which is provided by dockershim or containerd or cri-o.
kube-proxy - this provides ip address for pods, networking and default load balancing capabilities for 
the pods in the cluster.
Container Runtime - Pod needs container runtime to run, this can be dockershim, cri-o, containerd runtime.

Control plane:
API Server: API Server receives the input to create from the user decides which node to create the resouce on.
Scheduler: Scheduler takes the input from the API Server and send message to create the pod on the suggested node.
etcd: It stores all the information regarding the cluster in key value pairs.
controller manager: controller manager maintains the state of the pods like managing the replica sets.
cloud controller manager: for an cloud kubernetes service like gke, aks, eks. for the cluster to communicate with
the cloud services CCM is used.

why are we deploying pod on Kubernetes not container?
Pod.yml file contains all the specifications we need on a container. Instead of passing the commands in CLI
we create a pod file which contains the container and all the specs we need for it.
pod is one or more containers.

container vs pod vs deployment
container is a running application with the dependencies.
pod is a single or multiple containers along with the runnning specification.
deployment also deploys a pod but will provide the capabilities like Autoscaling, Autohealing and zero downtime capabilities.
deployment --> replicaset(controller) --> pods

deployment vs replicaset
deployment also deploys a pod but will provide the capabilities like Autoscaling, Autohealing and zero downtime capabilities.
replicaset is a kubernetes controller which maintains the state of the deployment.

services:
In prod scenario every deployment have a service.
Service can do loadbalancing, service discovery, expose application to external world.
Service are of 3 types:
	clusterIp --> still application is only accessible inside the cluster.
	nodeport --> allows the application to accessible out of the cluster by using the ip of the worker IP.
	loadbalancer --> application can be accessible in the external world with the public load balancer IP.

Ingress:
Services doesnot provide the enterprise level loadbalancing &
loadbalancer Service exposes public IP, it is not ecomical when an application have multiple services.
In Ingress user have to create Ingress resource, and the loadbalancing provider should create the Ingress
controller. loadbalancing providers are nginx, HAproxy, F5 etc.
User have to install Ingress controller on the kubernetes cluster. then Ingress resouce should be created.
Traffic flow with Ingress:
request --> Ingress controller --> Ingress resouce --> service --> pods
Ingress routing is of two types:
host based, path based
host based is like host: app1.myweb.com and app2.myweb.com
path based is like host: myweb.com path: /app1 and path: /app2

Ingress can take care of TLS:
there are three kinds of TLS:
SSL passthrough - request of https type pass through loadbalancer and the request is decrypted at service. 
SSL Offloading - request of https tpes pass through loadbalancer and get decrypted to http and this http flows to the service.
SSL Bridging - http requests coming to the LB will get decrypted and again re-encrypted and sent to the service.

Config Maps and Secrets:
Config Maps: kubernetes object to store non confidential data in key value pairs that can be passed to
pods at runtime.
configmaps can be attached to pods as environment variables or volume mounts.
if attached through environment variables the varibles cannot be changed.
when configmaps mounted as volume, the cms are available as files.

RBAC:
RBAC can be implemented for Users and Service Acounts.
1. Users/Service Accounts
2. Roles/ClusterRoles
3. Rolebinding/ClusterRolebinding

Kubernetes doesnot take care user management, it should be taken care by kubernetes service provider.

Resource Request - The amount of available cpu and memory on the node to schedule a pod. post 
deployment pod can use more or less than the resource request, this is only used for scheduling.
Requests are imposed at scheduling time.
Resource Limit - The amount of memory or cpu that a pod is allowed to use on the node.
Limits are imposed at runtime.
Limit Value >>> Request Value

Health Check:
Liveness probe - helps to determine container state. can be checked by running command in container, periodic http health check.
StartUp probe - StartUp probe runs during the container start time and stops once container started. once
				StartUp probe ends Liveness probe starts.
Readiness probe - this probe is to determine whether the pod is ready to accept the traffic.

Container restart policies - Always, OnFailure, Never
By default pod restart policy is Always even if pod exited successfully

Init container:
specialized containers that run in a pod before the app container.
this only works during startup of the pod.

Pods can be scheduled on different nodes based on the following:
1. nodeSelector -- using labels
2. nodeName -- using the node nodeName
3. affinity -- enhanced version of nodeSelector
4. anti-affinity -- when not to schedule pods on nodes, anti-affinity can be used.
	(check scheduling folder for examples)

DaemonSet -- To run a copy of a pod on each node created. whenever a new node created DaemonSet runs.
StaticPod -- Static pod is managed by kubelet on nodes. control plane not required to run static pods.
			every static pod creates a mirror pod on the master plane to monitor but cannot be managed.

storage:
volumes: volumes help to store data outside of the container. data replication happens in the realtime.
Persistent volumes: PV is a bit more advanced than the volumes. PV allows to treat volume as a resource and 
					can be consumed in using pods.
pod --> Persistent volume claim --> Persistent volumes --> external storage
volume can be made on: 
	NFS
	cloud storage
	Configmaps & Secrets
	File system on node
volumes in pod can be defined in two ways:
	hostPath & emptyDir
emptyDir: multiple containers can read and write same empty vol, it will cease to exist once pod is removed.

Persistent volume: storage as a resource. PV uses attributes to describe underlying storage resources.
PV can be created as a kind.
Flow of PVC:
Pod --> Persistanct Volume --> Persistanct Volume Claim --> Storage Class --> Storage